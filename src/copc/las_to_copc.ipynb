{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d420040",
   "metadata": {},
   "source": [
    "# LAS/LAZ to COPC Conversion Tutorial\n",
    "\n",
    "This notebook demonstrates how to convert LAS/LAZ point cloud files to COPC (Cloud Optimized Point Cloud) format using PDAL.\n",
    "\n",
    "## What is COPC?\n",
    "\n",
    "COPC (Cloud Optimized Point Cloud) adds spatial indexing to point cloud data, enabling:\n",
    "- **Fast spatial queries** - Extract regions without reading entire files\n",
    "- **Progressive loading** - Load data at different detail levels\n",
    "- **Better performance** - Especially for web applications and large datasets\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. **Install dependencies** using the provided environment file:\n",
    "   ```bash\n",
    "   conda env create -f environment.yml\n",
    "   conda activate copc_api\n",
    "   ```\n",
    "\n",
    "2. **Get a LAS/LAZ file** to work with (you can download from the README or use your own)\n",
    "\n",
    "> **Note**: PDAL installation may take several minutes - be patient!\n",
    "\n",
    "1. Have a las file that you want to work with and convert to copc\n",
    "2. (Optional) Export it from the hoydedata.no website\n",
    "3. Install the required packages for doing this conversion\n",
    "    Best way of doing this is by using the PDAL library which is most easily installed through Conda\n",
    "\n",
    "In this folder you can find the requirements.yml file which contains all of the packages needed for this notebook and can be created using the following command\n",
    "\n",
    "> conda env create -f environment.yml\n",
    "\n",
    "Installing the PDAL library may take some time, so be patient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c7ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb6532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-11 12:37:57--  https://github.com/PDAL/data/raw/refs/heads/main/isprs/CSite2_orig-utm.laz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/PDAL/data/refs/heads/main/isprs/CSite2_orig-utm.laz [following]\n",
      "--2025-08-11 12:37:57--  https://media.githubusercontent.com/media/PDAL/data/refs/heads/main/isprs/CSite2_orig-utm.laz\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1464552 (1,4M) [application/octet-stream]\n",
      "Saving to: ‘CSite2_orig-utm.laz’\n",
      "\n",
      "CSite2_orig-utm.laz 100%[===================>]   1,40M  --.-KB/s    in 0,05s   \n",
      "\n",
      "2025-08-11 12:37:58 (30,8 MB/s) - ‘CSite2_orig-utm.laz’ saved [1464552/1464552]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/PDAL/data/raw/refs/heads/main/isprs/CSite2_orig-utm.laz\n",
    "!mv CSite2_orig-utm.laz test.laz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10650638",
   "metadata": {},
   "source": [
    "## Step 1: Examine Your Point Cloud Data\n",
    "\n",
    "Let's first understand what we're working with by reading and analyzing the LAS/LAZ file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68f13e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19253870"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las_file = \"test_2.laz\"\n",
    "pipeline = pdal.Pipeline(json.dumps({\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"filename\": las_file\n",
    "        }\n",
    "    ]\n",
    "}))\n",
    "\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e11d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Points: 19,253,870\n",
      "  Dimensions: ['X', 'Y', 'Z', 'Intensity', 'ReturnNumber', 'NumberOfReturns', 'ScanDirectionFlag', 'EdgeOfFlightLine', 'Classification', 'Synthetic', 'KeyPoint', 'Withheld', 'Overlap', 'ScanAngleRank', 'UserData', 'PointSourceId', 'GpsTime', 'ScanChannel']\n",
      "  Bounds: X(453600.0 to 454400.0)\n",
      "           Y(6456000.0 to 6456600.0)\n",
      "           Z(57.6 to 162.1)\n",
      "  First point: X=453600.03, Y=6456486.00, Z=130.39\n"
     ]
    }
   ],
   "source": [
    "arrays = pipeline.arrays\n",
    "if arrays:\n",
    "    points = arrays[0]\n",
    "    print(f\"  Points: {len(points):,}\")\n",
    "    print(f\"  Dimensions: {list(points.dtype.names)}\")\n",
    "    print(f\"  Bounds: X({points['X'].min():.1f} to {points['X'].max():.1f})\")\n",
    "    print(f\"           Y({points['Y'].min():.1f} to {points['Y'].max():.1f})\")\n",
    "    print(f\"           Z({points['Z'].min():.1f} to {points['Z'].max():.1f})\")\n",
    "    \n",
    "    # Show first point\n",
    "    print(f\"  First point: X={points['X'][0]:.2f}, Y={points['Y'][0]:.2f}, Z={points['Z'][0]:.2f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c080a3",
   "metadata": {},
   "source": [
    "From the output above we can see that the laz file has approx. 500k points. There is also metainformation for each point such as \"Intensity\", \"Classification\", \"Withheld\" etc - which can vary depending on the data creation.\n",
    "\n",
    "Lets start by examining some of the data we have, e.g. the 5 first values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66195bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.void((453600.03, 6456486.0, 130.39000000000001, 49762, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0.57, 2, 19, 366634123.5912308, 2), dtype=[('X', '<f8'), ('Y', '<f8'), ('Z', '<f8'), ('Intensity', '<u2'), ('ReturnNumber', 'u1'), ('NumberOfReturns', 'u1'), ('ScanDirectionFlag', 'u1'), ('EdgeOfFlightLine', 'u1'), ('Classification', 'u1'), ('Synthetic', 'u1'), ('KeyPoint', 'u1'), ('Withheld', 'u1'), ('Overlap', 'u1'), ('ScanAngleRank', '<f4'), ('UserData', 'u1'), ('PointSourceId', '<u2'), ('GpsTime', '<f8'), ('ScanChannel', 'u1')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = pipeline.arrays[0][0]\n",
    "point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac41b70",
   "metadata": {},
   "source": [
    "This creates a point as a np.void object which is a numpy datatype for storing a structured element of an array. This allows us to index using the different keys that are stored with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a83d3614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dtypes in the point:  ['X', 'Y', 'Z', 'Intensity', 'ReturnNumber', 'NumberOfReturns', 'ScanDirectionFlag', 'EdgeOfFlightLine', 'Classification', 'Synthetic', 'KeyPoint', 'Withheld', 'Overlap', 'ScanAngleRank', 'UserData', 'PointSourceId', 'GpsTime', 'ScanChannel']\n",
      "X: 453600.03\n",
      "----------\n",
      "Y: 6456486.0\n",
      "----------\n",
      "Z: 130.39000000000001\n",
      "----------\n",
      "Intensity: 49762\n",
      "----------\n",
      "Classification: 1\n",
      "----------\n",
      "Withheld: 0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"All dtypes in the point: \", list(point.dtype.names))\n",
    "\n",
    "values_of_interest = [\"X\", \"Y\", \"Z\", \"Intensity\", \"Classification\", \"Withheld\"]\n",
    "\n",
    "for value in values_of_interest:\n",
    "    print(f\"{value}: {point[value]}\")\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d757c1",
   "metadata": {},
   "source": [
    "In this way we can inspect the values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50cbde7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Converting test_2.laz → compressed_output.copc (COPC, chunkSize=100000)\n",
      "✅ Wrote COPC: compressed_output.copc\n",
      "⏱️  Conversion time: 42.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'compressed_output.copc'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_laz_to_copc(input_file: str, output_file: str = \"output.copc.laz\", chunk_size: int = 100000):\n",
    "    \"\"\"Convert LAS/LAZ to COPC (LAZ-compressed) using PDAL's writers.copc.\"\"\"\n",
    "    \n",
    "    print(f\"🔄 Converting {input_file} → {output_file} (COPC, chunkSize={chunk_size})\")\n",
    "\n",
    "    pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": input_file\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.copc\",\n",
    "                \"filename\": output_file,\n",
    "                \"forward\": \"all\"\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "\n",
    "    start_time = time.time()\n",
    "    pipeline.execute()\n",
    "    conversion_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"✅ Wrote COPC: {output_file}\")\n",
    "    print(f\"⏱️  Conversion time: {conversion_time:.2f} s\")\n",
    "    return output_file\n",
    "\n",
    "# Example:\n",
    "convert_laz_to_copc(\"test_2.laz\", \"compressed_output.copc\", chunk_size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76507ff3",
   "metadata": {},
   "source": [
    "## Step 2: Convert to COPC Format\n",
    "\n",
    "The conversion was fast! Now let's explore the key differences between LAZ and COPC files.\n",
    "\n",
    "Let's highlight and look at the :\n",
    "1. File size\n",
    "2. Spatial indexing\n",
    "3. Progressive loading\n",
    "\n",
    "\n",
    "### File Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54993d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 File Size Comparison:\n",
      "  LAZ file:  118.52 MB (124,276,257 bytes)\n",
      "  COPC file: 115.41 MB (121,015,623 bytes)\n",
      "  Size ratio: 0.97x\n",
      "  ✅ COPC is 2.6% smaller than LAZ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'laz_size_mb': 118.51907444000244,\n",
       " 'copc_size_mb': 115.40949153900146,\n",
       " 'ratio': 0.9737630173396677}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_file_sizes(laz_file: str, copc_file: str):\n",
    "    \"\"\"Compare file sizes between LAZ and COPC files\"\"\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not Path(laz_file).exists():\n",
    "        print(f\"❌ LAZ file not found: {laz_file}\")\n",
    "        return\n",
    "    \n",
    "    if not Path(copc_file).exists():\n",
    "        print(f\"❌ COPC file not found: {copc_file}\")\n",
    "        return\n",
    "    \n",
    "    # Get file sizes in bytes\n",
    "    laz_size_bytes = Path(laz_file).stat().st_size\n",
    "    copc_size_bytes = Path(copc_file).stat().st_size\n",
    "    \n",
    "    # Convert to MB for display\n",
    "    laz_size_mb = laz_size_bytes / (1024**2)\n",
    "    copc_size_mb = copc_size_bytes / (1024**2)\n",
    "    \n",
    "    # Calculate ratio\n",
    "    size_ratio = copc_size_bytes / laz_size_bytes\n",
    "    \n",
    "    print(f\"📊 File Size Comparison:\")\n",
    "    print(f\"  LAZ file:  {laz_size_mb:.2f} MB ({laz_size_bytes:,} bytes)\")\n",
    "    print(f\"  COPC file: {copc_size_mb:.2f} MB ({copc_size_bytes:,} bytes)\")\n",
    "    print(f\"  Size ratio: {size_ratio:.2f}x\")\n",
    "    \n",
    "    if size_ratio < 1.0:\n",
    "        print(f\"  ✅ COPC is {((1-size_ratio)*100):.1f}% smaller than LAZ\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  COPC is {((size_ratio-1)*100):.1f}% larger than LAZ\")\n",
    "    \n",
    "    return {\n",
    "        'laz_size_mb': laz_size_mb,\n",
    "        'copc_size_mb': copc_size_mb,\n",
    "        'ratio': size_ratio\n",
    "    }\n",
    "\n",
    "compare_file_sizes(\"test_2.laz\", \"compressed_output.copc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f4637",
   "metadata": {},
   "source": [
    "As we can see here, the file size actually increases quite dramatically. The file size is 2.21 (!) times larger than the laz file. Though, as we will see in the rest of the notebook, the increased file size may introduce benefits that outweigh the increased file size.\n",
    "\n",
    "\n",
    "### Spatial Indexing Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dbae55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃‍♂️ Region Extraction Benchmark (region size: 100m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "��️  Region bounds: X(453600.0 to 453700.0), Y(6456000.0 to 6456100.0)\n",
      "\n",
      "📖 Extracting region from LAZ file...\n",
      "  ✅ Points extracted: 390,547\n",
      "  ⏱️  Time: 5.39 seconds\n",
      "  �� Speed: 72,494 points/second\n",
      "\n",
      "🎯 Extracting region from COPC file...\n",
      "  ✅ Points extracted: 390,547\n",
      "  ⏱️  Time: 0.45 seconds\n",
      "  🚀 Speed: 869,232 points/second\n",
      "\n",
      "📊 Performance Comparison:\n",
      "  🏆 COPC is 12.0x faster than LAZ\n",
      "  ⏱️  Time saved: 4.94 seconds\n",
      "  ✅ Same point count: 390,547\n"
     ]
    }
   ],
   "source": [
    "def benchmark_region_extraction(laz_file: str, copc_file: str, region_size: float = 100.0):\n",
    "    \"\"\"Benchmark extracting data from a specific region\"\"\"\n",
    "    \n",
    "    print(f\"🏃‍♂️ Region Extraction Benchmark (region size: {region_size}m)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, get the bounds from the COPC file to define a region\n",
    "    print(\"📊 Getting data bounds...\")\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.copc\",\n",
    "                \"filename\": copc_file\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    \n",
    "    metadata = copc_pipeline.metadata\n",
    "    \n",
    "    # Try different ways to get bounds\n",
    "    bounds = None\n",
    "    if 'metadata' in metadata and 'readers.copc' in metadata['metadata']:\n",
    "        copc_meta = metadata['metadata']['readers.copc']\n",
    "        if 'bounds' in copc_meta:\n",
    "            bounds = copc_meta['bounds']\n",
    "        elif 'minx' in copc_meta and 'maxx' in copc_meta:\n",
    "            bounds = {\n",
    "                'X': copc_meta['minx'],\n",
    "                'Y': copc_meta['miny'],\n",
    "                'Z': copc_meta['minz']\n",
    "            }\n",
    "    \n",
    "    # If still no bounds, use the actual data bounds\n",
    "    if bounds is None:\n",
    "        print(\"⚠️  Using data bounds instead of metadata bounds\")\n",
    "        points = copc_pipeline.arrays[0]\n",
    "        bounds = {\n",
    "            'X': float(points['X'].min()),\n",
    "            'Y': float(points['Y'].min()),\n",
    "            'Z': float(points['Z'].min())\n",
    "        }\n",
    "    \n",
    "    x_min, x_max = bounds['X'], bounds['X'] + region_size\n",
    "    y_min, y_max = bounds['Y'], bounds['Y'] + region_size\n",
    "    \n",
    "    print(f\"��️  Region bounds: X({x_min:.1f} to {x_max:.1f}), Y({y_min:.1f} to {y_max:.1f})\")\n",
    "    \n",
    "    # Test 1: Extract region from LAZ file (must read entire file)\n",
    "    print(f\"\\n📖 Extracting region from LAZ file...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": laz_file\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.crop\",\n",
    "                \"bounds\": f\"([{x_min},{x_max}],[{y_min},{y_max}])\"\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "    laz_pipeline.execute()\n",
    "    laz_points = len(laz_pipeline.arrays[0])\n",
    "    laz_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ✅ Points extracted: {laz_points:,}\")\n",
    "    print(f\"  ⏱️  Time: {laz_time:.2f} seconds\")\n",
    "    print(f\"  �� Speed: {laz_points/laz_time:,.0f} points/second\")\n",
    "    \n",
    "    # Test 2: Extract region from COPC file (spatial index)\n",
    "    print(f\"\\n🎯 Extracting region from COPC file...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    copc_region_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.copc\",\n",
    "                \"filename\": copc_file,\n",
    "                \"bounds\": f\"([{x_min},{x_max}],[{y_min},{y_max}])\"\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "    copc_region_pipeline.execute()\n",
    "    copc_points = len(copc_region_pipeline.arrays[0])\n",
    "    copc_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ✅ Points extracted: {copc_points:,}\")\n",
    "    print(f\"  ⏱️  Time: {copc_time:.2f} seconds\")\n",
    "    print(f\"  🚀 Speed: {copc_points/copc_time:,.0f} points/second\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n📊 Performance Comparison:\")\n",
    "    speed_improvement = laz_time / copc_time if copc_time > 0 else float('inf')\n",
    "    print(f\"  🏆 COPC is {speed_improvement:.1f}x faster than LAZ\")\n",
    "    print(f\"  ⏱️  Time saved: {laz_time - copc_time:.2f} seconds\")\n",
    "    \n",
    "    if laz_points != copc_points:\n",
    "        print(f\"  ⚠️  Point count difference: LAZ={laz_points:,}, COPC={copc_points:,}\")\n",
    "    else:\n",
    "        print(f\"  ✅ Same point count: {laz_points:,}\")\n",
    "    \n",
    "    return {\n",
    "        'laz': {'points': laz_points, 'time': laz_time},\n",
    "        'copc': {'points': copc_points, 'time': copc_time},\n",
    "        'speedup': speed_improvement\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    laz_file = \"test_2.laz\"\n",
    "    copc_file = \"compressed_output.copc\"\n",
    "    \n",
    "    # Single region benchmark\n",
    "    results = benchmark_region_extraction(laz_file, copc_file, region_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6133e0c",
   "metadata": {},
   "source": [
    "As we can see here, extracting regions in the files are much faster using the copc file. The main reason for this is the spatial indexing that allows us to only extract the data for a small subset of the file, while for the laz file we need to read the entire file in order to extract the same subset. This overhead increases even more when we need to extract multiple regions, especially from multiple files.\n",
    "\n",
    "### Extracting multiple regions from the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea2625c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 50m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(453600.0 to 454400.0), Y(6456000.0 to 6456600.0)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  101119 ± 26032 points in 5.07 ± 0.46s\n",
      "  COPC: 101119 ± 26032 points in 0.29 ± 0.09s\n",
      "  �� Average speedup: 17.6x\n",
      "  ⏱️  Average time saved: 4.78s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 17.6x faster on average\n",
      "  📈 Speedup range: 9.0x to 37.7x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 50m: COPC is 17.6x faster\n",
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 100m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(453600.0 to 454400.0), Y(6456000.0 to 6456600.0)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  430214 ± 68772 points in 5.11 ± 0.52s\n",
      "  COPC: 430214 ± 68772 points in 0.56 ± 0.08s\n",
      "  �� Average speedup: 9.1x\n",
      "  ⏱️  Average time saved: 4.55s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 9.1x faster on average\n",
      "  📈 Speedup range: 5.5x to 13.5x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 100m: COPC is 9.1x faster\n",
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 200m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(453600.0 to 454400.0), Y(6456000.0 to 6456600.0)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  1685828 ± 209018 points in 4.93 ± 0.61s\n",
      "  COPC: 1685828 ± 209018 points in 1.14 ± 0.22s\n",
      "  �� Average speedup: 4.3x\n",
      "  ⏱️  Average time saved: 3.78s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 4.3x faster on average\n",
      "  📈 Speedup range: 2.3x to 6.9x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 200m: COPC is 4.3x faster\n",
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 500m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(453600.0 to 454400.0), Y(6456000.0 to 6456600.0)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  10224374 ± 516905 points in 4.74 ± 0.43s\n",
      "  COPC: 10224374 ± 516905 points in 3.95 ± 0.39s\n",
      "  �� Average speedup: 1.2x\n",
      "  ⏱️  Average time saved: 0.79s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 1.2x faster on average\n",
      "  📈 Speedup range: 0.9x to 1.6x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 500m: COPC is 1.2x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_average_regions(laz_file: str, copc_file: str, num_regions: int = 10, region_size: float = 100.0):\n",
    "    \"\"\"Benchmark average performance across multiple random regions\"\"\"\n",
    "    \n",
    "    print(f\"🧪 Average Performance Across {num_regions} Regions (size: {region_size}m)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get bounds from COPC file\n",
    "    print(\"📊 Getting data bounds...\")\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\"type\": \"readers.copc\", \"filename\": copc_file}\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    points = copc_pipeline.arrays[0]\n",
    "    \n",
    "    x_min, x_max = float(points['X'].min()), float(points['X'].max())\n",
    "    y_min, y_max = float(points['Y'].min()), float(points['Y'].max())\n",
    "    \n",
    "    print(f\"📐 Full dataset bounds: X({x_min:.1f} to {x_max:.1f}), Y({y_min:.1f} to {y_max:.1f})\")\n",
    "    \n",
    "    # Store results for all regions\n",
    "    laz_times = []\n",
    "    copc_times = []\n",
    "    laz_points = []\n",
    "    copc_points = []\n",
    "    \n",
    "    print(f\"\\n🔄 Testing {num_regions} random regions...\")\n",
    "    \n",
    "    for i in range(num_regions):\n",
    "        # Generate random region within bounds\n",
    "        region_x_min = np.random.uniform(x_min, x_max - region_size)\n",
    "        region_y_min = np.random.uniform(y_min, y_max - region_size)\n",
    "        region_x_max = region_x_min + region_size\n",
    "        region_y_max = region_y_min + region_size\n",
    "        \n",
    "        # Test LAZ extraction\n",
    "        start_time = time.time()\n",
    "        laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\"type\": \"readers.las\", \"filename\": laz_file},\n",
    "                {\"type\": \"filters.crop\", \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"}\n",
    "            ]\n",
    "        }))\n",
    "        laz_pipeline.execute()\n",
    "        laz_time = time.time() - start_time\n",
    "        laz_points_count = len(laz_pipeline.arrays[0])\n",
    "        \n",
    "        # Test COPC extraction\n",
    "        start_time = time.time()\n",
    "        copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\n",
    "                    \"type\": \"readers.copc\",\n",
    "                    \"filename\": copc_file,\n",
    "                    \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"\n",
    "                }\n",
    "            ]\n",
    "        }))\n",
    "        copc_pipeline.execute()\n",
    "        copc_time = time.time() - start_time\n",
    "        copc_points_count = len(copc_pipeline.arrays[0])\n",
    "        \n",
    "        # Store results\n",
    "        laz_times.append(laz_time)\n",
    "        copc_times.append(copc_time)\n",
    "        laz_points.append(laz_points_count)\n",
    "        copc_points.append(copc_points_count)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Completed {i + 1}/{num_regions} regions...\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_laz_time = np.mean(laz_times)\n",
    "    avg_copc_time = np.mean(copc_times)\n",
    "    avg_laz_points = np.mean(laz_points)\n",
    "    avg_copc_points = np.mean(copc_points)\n",
    "    avg_speedup = avg_laz_time / avg_copc_time\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    std_laz_time = np.std(laz_times)\n",
    "    std_copc_time = np.std(copc_times)\n",
    "    \n",
    "    print(f\"\\n📊 Average Results Across {num_regions} Regions:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  LAZ:  {avg_laz_points:.0f} ± {np.std(laz_points):.0f} points in {avg_laz_time:.2f} ± {std_laz_time:.2f}s\")\n",
    "    print(f\"  COPC: {avg_copc_points:.0f} ± {np.std(copc_points):.0f} points in {avg_copc_time:.2f} ± {std_copc_time:.2f}s\")\n",
    "    print(f\"  �� Average speedup: {avg_speedup:.1f}x\")\n",
    "    print(f\"  ⏱️  Average time saved: {avg_laz_time - avg_copc_time:.2f}s per region\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n🎯 Performance Summary:\")\n",
    "    print(f\"  ✅ COPC is {avg_speedup:.1f}x faster on average\")\n",
    "    print(f\"  📈 Speedup range: {min(laz_times)/max(copc_times):.1f}x to {max(laz_times)/min(copc_times):.1f}x\")\n",
    "    print(f\"  🎲 Consistent performance: {'Yes' if std_copc_time/avg_copc_time < 0.5 else 'Variable'}\")\n",
    "    \n",
    "    return {\n",
    "        'avg_laz_time': avg_laz_time,\n",
    "        'avg_copc_time': avg_copc_time,\n",
    "        'avg_speedup': avg_speedup,\n",
    "        'std_laz_time': std_laz_time,\n",
    "        'std_copc_time': std_copc_time\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    laz_file = \"test_2.laz\"\n",
    "    copc_file = \"compressed_output.copc\"\n",
    "    \n",
    "    # Test with different region sizes\n",
    "    region_sizes = [50, 100, 200, 500]\n",
    "    \n",
    "    for size in region_sizes:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        results = benchmark_average_regions(laz_file, copc_file, num_regions=10, region_size=size)\n",
    "        print(f\"Region size {size}m: COPC is {results['avg_speedup']:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb47776",
   "metadata": {},
   "source": [
    "As we can see evident here, extracting a subset of data from the file is much faster using copc, especially when the areas are very small. The benefit decreases as the areas increase, but it is still substantially faster than just using the laz files.\n",
    "\n",
    "\n",
    "### Progressive Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b40fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Progressive Loading Benchmark (region size: 200.0m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📍 Region: X(453600.0 to 453800.0), Y(6456000.0 to 6456200.0)\n",
      "\n",
      "🔍 Testing different detail levels...\n",
      "--------------------------------------------------\n",
      "\n",
      "📏 Detail level: 0.5m resolution\n",
      "  LAZ extraction (full region)...\n",
      "  🎯 COPC extraction (resolution: 0.5m)...\n",
      "    LAZ:  1,675,265 points in 4.86s\n",
      "    COPC: 1,490,101 points in 0.95s\n",
      "    🏆 Speedup: 5.1x\n",
      "    📉 Data reduction: 11.1%\n",
      "\n",
      "📏 Detail level: 1.0m resolution\n",
      "  LAZ extraction (full region)...\n",
      "  🎯 COPC extraction (resolution: 1.0m)...\n",
      "    LAZ:  1,675,265 points in 5.60s\n",
      "    COPC: 686,571 points in 0.61s\n",
      "    🏆 Speedup: 9.1x\n",
      "    📉 Data reduction: 59.0%\n",
      "\n",
      "📏 Detail level: 2.0m resolution\n",
      "  LAZ extraction (full region)...\n",
      "  🎯 COPC extraction (resolution: 2.0m)...\n",
      "    LAZ:  1,675,265 points in 4.73s\n",
      "    COPC: 263,728 points in 0.44s\n",
      "    🏆 Speedup: 10.9x\n",
      "    📉 Data reduction: 84.3%\n",
      "\n",
      "📏 Detail level: 5.0m resolution\n",
      "  LAZ extraction (full region)...\n",
      "  🎯 COPC extraction (resolution: 5.0m)...\n",
      "    LAZ:  1,675,265 points in 4.24s\n",
      "    COPC: 76,971 points in 0.20s\n",
      "    🏆 Speedup: 21.7x\n",
      "    📉 Data reduction: 95.4%\n",
      "\n",
      "📏 Detail level: 10.0m resolution\n",
      "  LAZ extraction (full region)...\n",
      "  🎯 COPC extraction (resolution: 10.0m)...\n",
      "    LAZ:  1,675,265 points in 3.66s\n",
      "    COPC: 5,439 points in 0.05s\n",
      "    🏆 Speedup: 75.4x\n",
      "    📉 Data reduction: 99.7%\n",
      "\n",
      "📊 Progressive Loading Summary:\n",
      "================================================================================\n",
      "Resolution LAZ Points   LAZ Time   COPC Points  COPC Time  Speedup  Reduction \n",
      "--------------------------------------------------------------------------------\n",
      "0.5        1,675,265    4.86       1,490,101    0.95       5.1      11.1      %\n",
      "1.0        1,675,265    5.60       686,571      0.61       9.1      59.0      %\n",
      "2.0        1,675,265    4.73       263,728      0.44       10.9     84.3      %\n",
      "5.0        1,675,265    4.24       76,971       0.20       21.7     95.4      %\n",
      "10.0       1,675,265    3.66       5,439        0.05       75.4     99.7      %\n",
      "\n",
      "🎯 Detail Level Comparison\n",
      "============================================================\n",
      "📍 Region: 500.0m x 500.0m\n",
      "🎯 Testing three detail levels...\n",
      "\n",
      "📏 High Detail (0.5m resolution):\n",
      "  LAZ:  10,883,040 points in 6.11s\n",
      "  COPC: 9,734,515 points in 3.72s\n",
      "  🏆 Speedup: 1.6x\n",
      "  📉 Data reduction: 10.6%\n",
      "  ⚡ Time saved: 2.39s\n",
      "\n",
      "📏 Medium Detail (2.0m resolution):\n",
      "  LAZ:  10,883,040 points in 5.07s\n",
      "  COPC: 1,833,497 points in 0.88s\n",
      "  🏆 Speedup: 5.7x\n",
      "  📉 Data reduction: 83.2%\n",
      "  ⚡ Time saved: 4.19s\n",
      "\n",
      "📏 Low Detail (10.0m resolution):\n",
      "  LAZ:  10,883,040 points in 5.21s\n",
      "  COPC: 32,764 points in 0.05s\n",
      "  🏆 Speedup: 101.7x\n",
      "  📉 Data reduction: 99.7%\n",
      "  ⚡ Time saved: 5.15s\n",
      "\n",
      "🎯 Progressive Loading Benefits:\n",
      "==================================================\n",
      "  ✅ LAZ: Always loads full detail (no choice)\n",
      "  ✅ COPC: Can load different detail levels\n",
      "  🚀 Faster loading at lower detail levels\n",
      "  �� Perfect for mobile/web applications\n",
      "  🎮 Great for real-time visualization\n",
      "  💾 Bandwidth savings for web services\n",
      "  📊 Can start with overview, then add detail\n"
     ]
    }
   ],
   "source": [
    "def benchmark_progressive_loading(laz_file: str, copc_file: str, region_size: float = 200.0):\n",
    "    \"\"\"Benchmark progressive loading at different detail levels\"\"\"\n",
    "    \n",
    "    print(f\"📈 Progressive Loading Benchmark (region size: {region_size}m)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get bounds from COPC file\n",
    "    print(\"📊 Getting data bounds...\")\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\"type\": \"readers.copc\", \"filename\": copc_file}\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    points = copc_pipeline.arrays[0]\n",
    "    \n",
    "    x_min, x_max = float(points['X'].min()), float(points['X'].max())\n",
    "    y_min, y_max = float(points['Y'].min()), float(points['Y'].max())\n",
    "    \n",
    "    # Define region\n",
    "    region_x_min, region_x_max = x_min, x_min + region_size\n",
    "    region_y_min, region_y_max = y_min, y_min + region_size\n",
    "    \n",
    "    print(f\"📍 Region: X({region_x_min:.1f} to {region_x_max:.1f}), Y({region_y_min:.1f} to {region_y_max:.1f})\")\n",
    "    \n",
    "    # Test different detail levels (resolution)\n",
    "    detail_levels = [0.5, 1.0, 2.0, 5.0, 10.0]  # meters\n",
    "    \n",
    "    print(f\"\\n🔍 Testing different detail levels...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for resolution in detail_levels:\n",
    "        print(f\"\\n📏 Detail level: {resolution}m resolution\")\n",
    "        \n",
    "        # LAZ: Must read entire region (no progressive loading)\n",
    "        print(f\"  LAZ extraction (full region)...\")\n",
    "        start_time = time.time()\n",
    "        laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\"type\": \"readers.las\", \"filename\": laz_file},\n",
    "                {\"type\": \"filters.crop\", \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"}\n",
    "            ]\n",
    "        }))\n",
    "        laz_pipeline.execute()\n",
    "        laz_time = time.time() - start_time\n",
    "        laz_points = len(laz_pipeline.arrays[0])\n",
    "        \n",
    "        # COPC: Progressive loading at specific resolution\n",
    "        print(f\"  🎯 COPC extraction (resolution: {resolution}m)...\")\n",
    "        start_time = time.time()\n",
    "        copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\n",
    "                    \"type\": \"readers.copc\",\n",
    "                    \"filename\": copc_file,\n",
    "                    \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\",\n",
    "                    \"resolution\": resolution\n",
    "                }\n",
    "            ]\n",
    "        }))\n",
    "        copc_pipeline.execute()\n",
    "        copc_time = time.time() - start_time\n",
    "        copc_points = len(copc_pipeline.arrays[0])\n",
    "        \n",
    "        speedup = laz_time / copc_time if copc_time > 0 else float('inf')\n",
    "        data_reduction = (1 - copc_points/laz_points) * 100 if laz_points > 0 else 0\n",
    "        \n",
    "        print(f\"    LAZ:  {laz_points:,} points in {laz_time:.2f}s\")\n",
    "        print(f\"    COPC: {copc_points:,} points in {copc_time:.2f}s\")\n",
    "        print(f\"    🏆 Speedup: {speedup:.1f}x\")\n",
    "        print(f\"    📉 Data reduction: {data_reduction:.1f}%\")\n",
    "        \n",
    "        results.append({\n",
    "            'resolution': resolution,\n",
    "            'laz_points': laz_points,\n",
    "            'laz_time': laz_time,\n",
    "            'copc_points': copc_points,\n",
    "            'copc_time': copc_time,\n",
    "            'speedup': speedup,\n",
    "            'data_reduction': data_reduction\n",
    "        })\n",
    "    \n",
    "    # Summary table\n",
    "    print(f\"\\n📊 Progressive Loading Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Resolution':<10} {'LAZ Points':<12} {'LAZ Time':<10} {'COPC Points':<12} {'COPC Time':<10} {'Speedup':<8} {'Reduction':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"{result['resolution']:<10} {result['laz_points']:<12,} {result['laz_time']:<10.2f} \"\n",
    "              f\"{result['copc_points']:<12,} {result['copc_time']:<10.2f} {result['speedup']:<8.1f} \"\n",
    "              f\"{result['data_reduction']:<10.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def benchmark_detail_levels_comparison(laz_file: str, copc_file: str):\n",
    "    \"\"\"Compare different detail levels side by side\"\"\"\n",
    "    \n",
    "    print(f\"\\n🎯 Detail Level Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test with a larger region to see more dramatic differences\n",
    "    region_size = 500.0\n",
    "    \n",
    "    # Get bounds\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\"type\": \"readers.copc\", \"filename\": copc_file}\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    points = copc_pipeline.arrays[0]\n",
    "    \n",
    "    x_min, x_max = float(points['X'].min()), float(points['X'].max())\n",
    "    y_min, y_max = float(points['Y'].min()), float(points['Y'].max())\n",
    "    \n",
    "    region_x_min, region_x_max = x_min, x_min + region_size\n",
    "    region_y_min, region_y_max = y_min, y_min + region_size\n",
    "    \n",
    "    # Test three detail levels: high, medium, low\n",
    "    detail_levels = [\n",
    "        (\"High Detail\", 0.5),\n",
    "        (\"Medium Detail\", 2.0),\n",
    "        (\"Low Detail\", 10.0)\n",
    "    ]\n",
    "    \n",
    "    print(f\"📍 Region: {region_size}m x {region_size}m\")\n",
    "    print(f\"🎯 Testing three detail levels...\")\n",
    "    \n",
    "    for detail_name, resolution in detail_levels:\n",
    "        print(f\"\\n📏 {detail_name} ({resolution}m resolution):\")\n",
    "        \n",
    "        # LAZ (always full detail)\n",
    "        start_time = time.time()\n",
    "        laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\"type\": \"readers.las\", \"filename\": laz_file},\n",
    "                {\"type\": \"filters.crop\", \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"}\n",
    "            ]\n",
    "        }))\n",
    "        laz_pipeline.execute()\n",
    "        laz_time = time.time() - start_time\n",
    "        laz_points = len(laz_pipeline.arrays[0])\n",
    "        \n",
    "        # COPC (progressive detail)\n",
    "        start_time = time.time()\n",
    "        copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\n",
    "                    \"type\": \"readers.copc\",\n",
    "                    \"filename\": copc_file,\n",
    "                    \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\",\n",
    "                    \"resolution\": resolution\n",
    "                }\n",
    "            ]\n",
    "        }))\n",
    "        copc_pipeline.execute()\n",
    "        copc_time = time.time() - start_time\n",
    "        copc_points = len(copc_pipeline.arrays[0])\n",
    "        \n",
    "        speedup = laz_time / copc_time if copc_time > 0 else float('inf')\n",
    "        data_reduction = (1 - copc_points/laz_points) * 100 if laz_points > 0 else 0\n",
    "        \n",
    "        print(f\"  LAZ:  {laz_points:,} points in {laz_time:.2f}s\")\n",
    "        print(f\"  COPC: {copc_points:,} points in {copc_time:.2f}s\")\n",
    "        print(f\"  🏆 Speedup: {speedup:.1f}x\")\n",
    "        print(f\"  📉 Data reduction: {data_reduction:.1f}%\")\n",
    "        print(f\"  ⚡ Time saved: {laz_time - copc_time:.2f}s\")\n",
    "\n",
    "def show_progressive_benefits():\n",
    "    \"\"\"Show the benefits of progressive loading\"\"\"\n",
    "    print(f\"\\n🎯 Progressive Loading Benefits:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  ✅ LAZ: Always loads full detail (no choice)\")\n",
    "    print(f\"  ✅ COPC: Can load different detail levels\")\n",
    "    print(f\"  🚀 Faster loading at lower detail levels\")\n",
    "    print(f\"  �� Perfect for mobile/web applications\")\n",
    "    print(f\"  🎮 Great for real-time visualization\")\n",
    "    print(f\"  💾 Bandwidth savings for web services\")\n",
    "    print(f\"  📊 Can start with overview, then add detail\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    laz_file = \"test_2.laz\"\n",
    "    copc_file = \"compressed_output.copc\"\n",
    "    \n",
    "    # Progressive loading benchmark\n",
    "    progressive_results = benchmark_progressive_loading(laz_file, copc_file)\n",
    "    \n",
    "    # Detail level comparison\n",
    "    benchmark_detail_levels_comparison(laz_file, copc_file)\n",
    "    \n",
    "    # Show benefits\n",
    "    show_progressive_benefits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80b508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
