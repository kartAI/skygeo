{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d420040",
   "metadata": {},
   "source": [
    "### This is a notebook for showing how to go from the las (Point Cloud) to the copc (Cloud Optimized Point Cloud)\n",
    "\n",
    "1. Have a las file that you want to work with and convert to copc\n",
    "2. (Optional) Export it from the hoydedata.no website\n",
    "3. Install the required packages for doing this conversion\n",
    "    Best way of doing this is by using the PDAL library which is most easily installed through Conda\n",
    "\n",
    "In this folder you can find the requirements.yml file which contains all of the packages needed for this notebook and can be created using the following command\n",
    "\n",
    "> conda env create -f environment.yml\n",
    "\n",
    "Installing the PDAL library may take some time, so be patient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57c7ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10650638",
   "metadata": {},
   "source": [
    "Now that we have installed the required packages we can start by examining the las data we have, just so that we know what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68f13e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486800"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "las_file = \"test.laz\"\n",
    "pipeline = pdal.Pipeline(json.dumps({\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"filename\": las_file\n",
    "        }\n",
    "    ]\n",
    "}))\n",
    "\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34e11d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Points: 486,800\n",
      "  Dimensions: ['X', 'Y', 'Z', 'Intensity', 'ReturnNumber', 'NumberOfReturns', 'ScanDirectionFlag', 'EdgeOfFlightLine', 'Classification', 'Synthetic', 'KeyPoint', 'Withheld', 'Overlap', 'ScanAngleRank', 'UserData', 'PointSourceId']\n",
      "  Bounds: X(513449.7 to 513870.3)\n",
      "           Y(5402649.9 to 5403282.9)\n",
      "           Z(248.4 to 482.6)\n",
      "  First point: X=513450.03, Y=5402650.22, Z=296.38\n"
     ]
    }
   ],
   "source": [
    "arrays = pipeline.arrays\n",
    "if arrays:\n",
    "    points = arrays[0]\n",
    "    print(f\"  Points: {len(points):,}\")\n",
    "    print(f\"  Dimensions: {list(points.dtype.names)}\")\n",
    "    print(f\"  Bounds: X({points['X'].min():.1f} to {points['X'].max():.1f})\")\n",
    "    print(f\"           Y({points['Y'].min():.1f} to {points['Y'].max():.1f})\")\n",
    "    print(f\"           Z({points['Z'].min():.1f} to {points['Z'].max():.1f})\")\n",
    "    \n",
    "    # Show first point\n",
    "    print(f\"  First point: X={points['X'][0]:.2f}, Y={points['Y'][0]:.2f}, Z={points['Z'][0]:.2f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c080a3",
   "metadata": {},
   "source": [
    "From the output above we can see that the laz file has approx. 500k points. There is also metainformation for each point such as \"Intensity\", \"Classification\", \"Withheld\" etc - which can vary depending on the data creation.\n",
    "\n",
    "Lets start by examining some of the data we have, e.g. the 5 first values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66195bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.void((513450.03, 5402650.22, 296.38, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0), dtype=[('X', '<f8'), ('Y', '<f8'), ('Z', '<f8'), ('Intensity', '<u2'), ('ReturnNumber', 'u1'), ('NumberOfReturns', 'u1'), ('ScanDirectionFlag', 'u1'), ('EdgeOfFlightLine', 'u1'), ('Classification', 'u1'), ('Synthetic', 'u1'), ('KeyPoint', 'u1'), ('Withheld', 'u1'), ('Overlap', 'u1'), ('ScanAngleRank', '<f4'), ('UserData', 'u1'), ('PointSourceId', '<u2')])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = pipeline.arrays[0][0]\n",
    "point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac41b70",
   "metadata": {},
   "source": [
    "This creates a point as a np.void object which is a numpy datatype for storing a structured element of an array. This allows us to index using the different keys that are stored with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a83d3614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dtypes in the point:  ['X', 'Y', 'Z', 'Intensity', 'ReturnNumber', 'NumberOfReturns', 'ScanDirectionFlag', 'EdgeOfFlightLine', 'Classification', 'Synthetic', 'KeyPoint', 'Withheld', 'Overlap', 'ScanAngleRank', 'UserData', 'PointSourceId']\n",
      "X: 513450.03\n",
      "----------\n",
      "Y: 5402650.22\n",
      "----------\n",
      "Z: 296.38\n",
      "----------\n",
      "Intensity: 3\n",
      "----------\n",
      "Classification: 0\n",
      "----------\n",
      "Withheld: 0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"All dtypes in the point: \", list(point.dtype.names))\n",
    "\n",
    "values_of_interest = [\"X\", \"Y\", \"Z\", \"Intensity\", \"Classification\", \"Withheld\"]\n",
    "\n",
    "for value in values_of_interest:\n",
    "    print(f\"{value}: {point[value]}\")\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d757c1",
   "metadata": {},
   "source": [
    "In this way we can inspect the values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50cbde7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Converting test.laz to COPC...\n",
      "✅ Converted to: copc_output.copc\n",
      "⏱️  Conversion time: 0.52 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'copc_output.copc'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_laz_to_copc(input_file: str, output_file: str = \"copc_output.copc\"):\n",
    "    \"\"\"Convert LAZ file to COPC format\"\"\"\n",
    "    if output_file is None:\n",
    "        input_path = Path(input_file)\n",
    "        output_file = str(input_path.parent / f\"{input_path.stem}.copc.laz\")\n",
    "    \n",
    "    print(f\"🔄 Converting {input_file} to COPC...\")\n",
    "    \n",
    "    # PDAL pipeline for COPC conversion\n",
    "    pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": input_file\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.copc\",\n",
    "                \"filename\": output_file,\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pipeline.execute()\n",
    "    conversion_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"✅ Converted to: {output_file}\")\n",
    "    print(f\"⏱️  Conversion time: {conversion_time:.2f} seconds\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "convert_laz_to_copc(\"test.laz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76507ff3",
   "metadata": {},
   "source": [
    "As seen, the conversion is super fast. Now let's explore the differences we get from having a copc file instead of a laz file.\n",
    "\n",
    "Let's highlight and look at the :\n",
    "1. File size\n",
    "2. Spatial indexing\n",
    "3. Progressive loading\n",
    "\n",
    "\n",
    "### First, lets look at the file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54993d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 File Size Comparison:\n",
      "  LAZ file:  1.40 MB (1,464,552 bytes)\n",
      "  COPC file: 3.08 MB (3,233,818 bytes)\n",
      "  Size ratio: 2.21x\n",
      "  ⚠️  COPC is 120.8% larger than LAZ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'laz_size_mb': 1.3967056274414062,\n",
       " 'copc_size_mb': 3.0840091705322266,\n",
       " 'ratio': 2.2080595294670315}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_file_sizes(laz_file: str, copc_file: str):\n",
    "    \"\"\"Compare file sizes between LAZ and COPC files\"\"\"\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not Path(laz_file).exists():\n",
    "        print(f\"❌ LAZ file not found: {laz_file}\")\n",
    "        return\n",
    "    \n",
    "    if not Path(copc_file).exists():\n",
    "        print(f\"❌ COPC file not found: {copc_file}\")\n",
    "        return\n",
    "    \n",
    "    # Get file sizes in bytes\n",
    "    laz_size_bytes = Path(laz_file).stat().st_size\n",
    "    copc_size_bytes = Path(copc_file).stat().st_size\n",
    "    \n",
    "    # Convert to MB for display\n",
    "    laz_size_mb = laz_size_bytes / (1024**2)\n",
    "    copc_size_mb = copc_size_bytes / (1024**2)\n",
    "    \n",
    "    # Calculate ratio\n",
    "    size_ratio = copc_size_bytes / laz_size_bytes\n",
    "    \n",
    "    print(f\"📊 File Size Comparison:\")\n",
    "    print(f\"  LAZ file:  {laz_size_mb:.2f} MB ({laz_size_bytes:,} bytes)\")\n",
    "    print(f\"  COPC file: {copc_size_mb:.2f} MB ({copc_size_bytes:,} bytes)\")\n",
    "    print(f\"  Size ratio: {size_ratio:.2f}x\")\n",
    "    \n",
    "    if size_ratio < 1.0:\n",
    "        print(f\"  ✅ COPC is {((1-size_ratio)*100):.1f}% smaller than LAZ\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  COPC is {((size_ratio-1)*100):.1f}% larger than LAZ\")\n",
    "    \n",
    "    return {\n",
    "        'laz_size_mb': laz_size_mb,\n",
    "        'copc_size_mb': copc_size_mb,\n",
    "        'ratio': size_ratio\n",
    "    }\n",
    "\n",
    "compare_file_sizes(\"test.laz\", \"copc_output.copc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f4637",
   "metadata": {},
   "source": [
    "As we can see here, the file size actually increases quite dramatically. The file size is 2.21 (!) times larger than the laz file. Though, as we will see in the rest of the notebook, the increased file size may introduce benefits that outweigh the increased file size.\n",
    "\n",
    "\n",
    "### Secondly, let's have a look at the spatial indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbae55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃‍♂️ Region Extraction Benchmark (region size: 100m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "��️  Region bounds: X(513449.7 to 513549.7), Y(5402649.9 to 5402749.9)\n",
      "\n",
      "📖 Extracting region from LAZ file...\n",
      "  ✅ Points extracted: 19,612\n",
      "  ⏱️  Time: 0.10 seconds\n",
      "  �� Speed: 187,168 points/second\n",
      "\n",
      "🎯 Extracting region from COPC file...\n",
      "  ✅ Points extracted: 19,612\n",
      "  ⏱️  Time: 0.02 seconds\n",
      "  🚀 Speed: 822,727 points/second\n",
      "\n",
      "📊 Performance Comparison:\n",
      "  🏆 COPC is 4.4x faster than LAZ\n",
      "  ⏱️  Time saved: 0.08 seconds\n",
      "  ✅ Same point count: 19,612\n"
     ]
    }
   ],
   "source": [
    "def benchmark_region_extraction(laz_file: str, copc_file: str, region_size: float = 100.0):\n",
    "    \"\"\"Benchmark extracting data from a specific region\"\"\"\n",
    "    \n",
    "    print(f\"🏃‍♂️ Region Extraction Benchmark (region size: {region_size}m)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, get the bounds from the COPC file to define a region\n",
    "    print(\"📊 Getting data bounds...\")\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.copc\",\n",
    "                \"filename\": copc_file\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    \n",
    "    metadata = copc_pipeline.metadata\n",
    "    \n",
    "    # Try different ways to get bounds\n",
    "    bounds = None\n",
    "    if 'metadata' in metadata and 'readers.copc' in metadata['metadata']:\n",
    "        copc_meta = metadata['metadata']['readers.copc']\n",
    "        if 'bounds' in copc_meta:\n",
    "            bounds = copc_meta['bounds']\n",
    "        elif 'minx' in copc_meta and 'maxx' in copc_meta:\n",
    "            bounds = {\n",
    "                'X': copc_meta['minx'],\n",
    "                'Y': copc_meta['miny'],\n",
    "                'Z': copc_meta['minz']\n",
    "            }\n",
    "    \n",
    "    # If still no bounds, use the actual data bounds\n",
    "    if bounds is None:\n",
    "        print(\"⚠️  Using data bounds instead of metadata bounds\")\n",
    "        points = copc_pipeline.arrays[0]\n",
    "        bounds = {\n",
    "            'X': float(points['X'].min()),\n",
    "            'Y': float(points['Y'].min()),\n",
    "            'Z': float(points['Z'].min())\n",
    "        }\n",
    "    \n",
    "    x_min, x_max = bounds['X'], bounds['X'] + region_size\n",
    "    y_min, y_max = bounds['Y'], bounds['Y'] + region_size\n",
    "    \n",
    "    print(f\"��️  Region bounds: X({x_min:.1f} to {x_max:.1f}), Y({y_min:.1f} to {y_max:.1f})\")\n",
    "    \n",
    "    # Test 1: Extract region from LAZ file (must read entire file)\n",
    "    print(f\"\\n📖 Extracting region from LAZ file...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": laz_file\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.crop\",\n",
    "                \"bounds\": f\"([{x_min},{x_max}],[{y_min},{y_max}])\"\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "    laz_pipeline.execute()\n",
    "    laz_points = len(laz_pipeline.arrays[0])\n",
    "    laz_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ✅ Points extracted: {laz_points:,}\")\n",
    "    print(f\"  ⏱️  Time: {laz_time:.2f} seconds\")\n",
    "    print(f\"  �� Speed: {laz_points/laz_time:,.0f} points/second\")\n",
    "    \n",
    "    # Test 2: Extract region from COPC file (spatial index)\n",
    "    print(f\"\\n🎯 Extracting region from COPC file...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    copc_region_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.copc\",\n",
    "                \"filename\": copc_file,\n",
    "                \"bounds\": f\"([{x_min},{x_max}],[{y_min},{y_max}])\"\n",
    "            }\n",
    "        ]\n",
    "    }))\n",
    "    copc_region_pipeline.execute()\n",
    "    copc_points = len(copc_region_pipeline.arrays[0])\n",
    "    copc_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ✅ Points extracted: {copc_points:,}\")\n",
    "    print(f\"  ⏱️  Time: {copc_time:.2f} seconds\")\n",
    "    print(f\"  🚀 Speed: {copc_points/copc_time:,.0f} points/second\")\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n📊 Performance Comparison:\")\n",
    "    speed_improvement = laz_time / copc_time if copc_time > 0 else float('inf')\n",
    "    print(f\"  🏆 COPC is {speed_improvement:.1f}x faster than LAZ\")\n",
    "    print(f\"  ⏱️  Time saved: {laz_time - copc_time:.2f} seconds\")\n",
    "    \n",
    "    if laz_points != copc_points:\n",
    "        print(f\"  ⚠️  Point count difference: LAZ={laz_points:,}, COPC={copc_points:,}\")\n",
    "    else:\n",
    "        print(f\"  ✅ Same point count: {laz_points:,}\")\n",
    "    \n",
    "    return {\n",
    "        'laz': {'points': laz_points, 'time': laz_time},\n",
    "        'copc': {'points': copc_points, 'time': copc_time},\n",
    "        'speedup': speed_improvement\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    laz_file = \"test.laz\"\n",
    "    copc_file = \"copc_output.copc\"\n",
    "    \n",
    "    # Single region benchmark\n",
    "    results = benchmark_region_extraction(laz_file, copc_file, region_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6133e0c",
   "metadata": {},
   "source": [
    "As we can see here, extracting regions in the files are much faster using the copc file. The main reason for this is the spatial indexing that allows us to only extract the data for a small subset of the file, while for the laz file we need to read the entire file in order to extract the same subset. This overhead increases even more when we need to extract multiple regions, especially from multiple files.\n",
    "\n",
    "### Extracting multiple regions from the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea2625c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 50m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(513449.7 to 513870.3), Y(5402649.9 to 5403282.9)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  3900 ± 1166 points in 0.10 ± 0.00s\n",
      "  COPC: 3900 ± 1166 points in 0.03 ± 0.00s\n",
      "  �� Average speedup: 3.8x\n",
      "  ⏱️  Average time saved: 0.07s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 3.8x faster on average\n",
      "  📈 Speedup range: 3.1x to 5.0x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 50m: COPC is 3.8x faster\n",
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 100m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(513449.7 to 513870.3), Y(5402649.9 to 5403282.9)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  16140 ± 3021 points in 0.10 ± 0.01s\n",
      "  COPC: 16140 ± 3021 points in 0.04 ± 0.01s\n",
      "  �� Average speedup: 2.7x\n",
      "  ⏱️  Average time saved: 0.06s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 2.7x faster on average\n",
      "  📈 Speedup range: 1.4x to 5.9x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 100m: COPC is 2.7x faster\n",
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 200m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(513449.7 to 513870.3), Y(5402649.9 to 5403282.9)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  73614 ± 5857 points in 0.09 ± 0.01s\n",
      "  COPC: 73614 ± 5857 points in 0.06 ± 0.01s\n",
      "  �� Average speedup: 1.6x\n",
      "  ⏱️  Average time saved: 0.03s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 1.6x faster on average\n",
      "  📈 Speedup range: 1.0x to 2.8x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 200m: COPC is 1.6x faster\n",
      "\n",
      "======================================================================\n",
      "🧪 Average Performance Across 10 Regions (size: 500m)\n",
      "============================================================\n",
      "📊 Getting data bounds...\n",
      "📐 Full dataset bounds: X(513449.7 to 513870.3), Y(5402649.9 to 5403282.9)\n",
      "\n",
      "🔄 Testing 10 random regions...\n",
      "  Completed 5/10 regions...\n",
      "  Completed 10/10 regions...\n",
      "\n",
      "📊 Average Results Across 10 Regions:\n",
      "==================================================\n",
      "  LAZ:  371587 ± 12947 points in 0.10 ± 0.01s\n",
      "  COPC: 371587 ± 12947 points in 0.10 ± 0.01s\n",
      "  �� Average speedup: 1.0x\n",
      "  ⏱️  Average time saved: 0.00s per region\n",
      "\n",
      "🎯 Performance Summary:\n",
      "  ✅ COPC is 1.0x faster on average\n",
      "  📈 Speedup range: 0.8x to 1.4x\n",
      "  🎲 Consistent performance: Yes\n",
      "Region size 500m: COPC is 1.0x faster\n"
     ]
    }
   ],
   "source": [
    "def benchmark_average_regions(laz_file: str, copc_file: str, num_regions: int = 10, region_size: float = 100.0):\n",
    "    \"\"\"Benchmark average performance across multiple random regions\"\"\"\n",
    "    \n",
    "    print(f\"🧪 Average Performance Across {num_regions} Regions (size: {region_size}m)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get bounds from COPC file\n",
    "    print(\"📊 Getting data bounds...\")\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\"type\": \"readers.copc\", \"filename\": copc_file}\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    points = copc_pipeline.arrays[0]\n",
    "    \n",
    "    x_min, x_max = float(points['X'].min()), float(points['X'].max())\n",
    "    y_min, y_max = float(points['Y'].min()), float(points['Y'].max())\n",
    "    \n",
    "    print(f\"📐 Full dataset bounds: X({x_min:.1f} to {x_max:.1f}), Y({y_min:.1f} to {y_max:.1f})\")\n",
    "    \n",
    "    # Store results for all regions\n",
    "    laz_times = []\n",
    "    copc_times = []\n",
    "    laz_points = []\n",
    "    copc_points = []\n",
    "    \n",
    "    print(f\"\\n🔄 Testing {num_regions} random regions...\")\n",
    "    \n",
    "    for i in range(num_regions):\n",
    "        # Generate random region within bounds\n",
    "        region_x_min = np.random.uniform(x_min, x_max - region_size)\n",
    "        region_y_min = np.random.uniform(y_min, y_max - region_size)\n",
    "        region_x_max = region_x_min + region_size\n",
    "        region_y_max = region_y_min + region_size\n",
    "        \n",
    "        # Test LAZ extraction\n",
    "        start_time = time.time()\n",
    "        laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\"type\": \"readers.las\", \"filename\": laz_file},\n",
    "                {\"type\": \"filters.crop\", \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"}\n",
    "            ]\n",
    "        }))\n",
    "        laz_pipeline.execute()\n",
    "        laz_time = time.time() - start_time\n",
    "        laz_points_count = len(laz_pipeline.arrays[0])\n",
    "        \n",
    "        # Test COPC extraction\n",
    "        start_time = time.time()\n",
    "        copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\n",
    "                    \"type\": \"readers.copc\",\n",
    "                    \"filename\": copc_file,\n",
    "                    \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"\n",
    "                }\n",
    "            ]\n",
    "        }))\n",
    "        copc_pipeline.execute()\n",
    "        copc_time = time.time() - start_time\n",
    "        copc_points_count = len(copc_pipeline.arrays[0])\n",
    "        \n",
    "        # Store results\n",
    "        laz_times.append(laz_time)\n",
    "        copc_times.append(copc_time)\n",
    "        laz_points.append(laz_points_count)\n",
    "        copc_points.append(copc_points_count)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Completed {i + 1}/{num_regions} regions...\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_laz_time = np.mean(laz_times)\n",
    "    avg_copc_time = np.mean(copc_times)\n",
    "    avg_laz_points = np.mean(laz_points)\n",
    "    avg_copc_points = np.mean(copc_points)\n",
    "    avg_speedup = avg_laz_time / avg_copc_time\n",
    "    \n",
    "    # Calculate standard deviations\n",
    "    std_laz_time = np.std(laz_times)\n",
    "    std_copc_time = np.std(copc_times)\n",
    "    \n",
    "    print(f\"\\n📊 Average Results Across {num_regions} Regions:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  LAZ:  {avg_laz_points:.0f} ± {np.std(laz_points):.0f} points in {avg_laz_time:.2f} ± {std_laz_time:.2f}s\")\n",
    "    print(f\"  COPC: {avg_copc_points:.0f} ± {np.std(copc_points):.0f} points in {avg_copc_time:.2f} ± {std_copc_time:.2f}s\")\n",
    "    print(f\"  �� Average speedup: {avg_speedup:.1f}x\")\n",
    "    print(f\"  ⏱️  Average time saved: {avg_laz_time - avg_copc_time:.2f}s per region\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n🎯 Performance Summary:\")\n",
    "    print(f\"  ✅ COPC is {avg_speedup:.1f}x faster on average\")\n",
    "    print(f\"  📈 Speedup range: {min(laz_times)/max(copc_times):.1f}x to {max(laz_times)/min(copc_times):.1f}x\")\n",
    "    print(f\"  🎲 Consistent performance: {'Yes' if std_copc_time/avg_copc_time < 0.5 else 'Variable'}\")\n",
    "    \n",
    "    return {\n",
    "        'avg_laz_time': avg_laz_time,\n",
    "        'avg_copc_time': avg_copc_time,\n",
    "        'avg_speedup': avg_speedup,\n",
    "        'std_laz_time': std_laz_time,\n",
    "        'std_copc_time': std_copc_time\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    laz_file = \"test.laz\"\n",
    "    copc_file = \"copc_output.copc\"\n",
    "    \n",
    "    # Test with different region sizes\n",
    "    region_sizes = [50, 100, 200, 500]\n",
    "    \n",
    "    for size in region_sizes:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        results = benchmark_average_regions(laz_file, copc_file, num_regions=10, region_size=size)\n",
    "        print(f\"Region size {size}m: COPC is {results['avg_speedup']:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb47776",
   "metadata": {},
   "source": [
    "As we can see evident here, extracting a subset of data from the file is much faster using copc, especially when the areas are very small. The benefit decreases as the areas increase, but it is still substantially faster than just using the laz files.\n",
    "\n",
    "\n",
    "### Thirdly, let's have a look at the progressive loading between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_progressive_loading(laz_file: str, copc_file: str, region_size: float = 200.0):\n",
    "    \"\"\"Benchmark progressive loading at different detail levels\"\"\"\n",
    "    \n",
    "    print(f\"📈 Progressive Loading Benchmark (region size: {region_size}m)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get bounds from COPC file\n",
    "    print(\"📊 Getting data bounds...\")\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\"type\": \"readers.copc\", \"filename\": copc_file}\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    points = copc_pipeline.arrays[0]\n",
    "    \n",
    "    x_min, x_max = float(points['X'].min()), float(points['X'].max())\n",
    "    y_min, y_max = float(points['Y'].min()), float(points['Y'].max())\n",
    "    \n",
    "    # Define region\n",
    "    region_x_min, region_x_max = x_min, x_min + region_size\n",
    "    region_y_min, region_y_max = y_min, y_min + region_size\n",
    "    \n",
    "    print(f\"📍 Region: X({region_x_min:.1f} to {region_x_max:.1f}), Y({region_y_min:.1f} to {region_y_max:.1f})\")\n",
    "    \n",
    "    # Test different detail levels (resolution)\n",
    "    detail_levels = [0.5, 1.0, 2.0, 5.0, 10.0]  # meters\n",
    "    \n",
    "    print(f\"\\n🔍 Testing different detail levels...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for resolution in detail_levels:\n",
    "        print(f\"\\n📏 Detail level: {resolution}m resolution\")\n",
    "        \n",
    "        # LAZ: Must read entire region (no progressive loading)\n",
    "        print(f\"  LAZ extraction (full region)...\")\n",
    "        start_time = time.time()\n",
    "        laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\"type\": \"readers.las\", \"filename\": laz_file},\n",
    "                {\"type\": \"filters.crop\", \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"}\n",
    "            ]\n",
    "        }))\n",
    "        laz_pipeline.execute()\n",
    "        laz_time = time.time() - start_time\n",
    "        laz_points = len(laz_pipeline.arrays[0])\n",
    "        \n",
    "        # COPC: Progressive loading at specific resolution\n",
    "        print(f\"  🎯 COPC extraction (resolution: {resolution}m)...\")\n",
    "        start_time = time.time()\n",
    "        copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\n",
    "                    \"type\": \"readers.copc\",\n",
    "                    \"filename\": copc_file,\n",
    "                    \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\",\n",
    "                    \"resolution\": resolution\n",
    "                }\n",
    "            ]\n",
    "        }))\n",
    "        copc_pipeline.execute()\n",
    "        copc_time = time.time() - start_time\n",
    "        copc_points = len(copc_pipeline.arrays[0])\n",
    "        \n",
    "        speedup = laz_time / copc_time if copc_time > 0 else float('inf')\n",
    "        data_reduction = (1 - copc_points/laz_points) * 100 if laz_points > 0 else 0\n",
    "        \n",
    "        print(f\"    LAZ:  {laz_points:,} points in {laz_time:.2f}s\")\n",
    "        print(f\"    COPC: {copc_points:,} points in {copc_time:.2f}s\")\n",
    "        print(f\"    🏆 Speedup: {speedup:.1f}x\")\n",
    "        print(f\"    📉 Data reduction: {data_reduction:.1f}%\")\n",
    "        \n",
    "        results.append({\n",
    "            'resolution': resolution,\n",
    "            'laz_points': laz_points,\n",
    "            'laz_time': laz_time,\n",
    "            'copc_points': copc_points,\n",
    "            'copc_time': copc_time,\n",
    "            'speedup': speedup,\n",
    "            'data_reduction': data_reduction\n",
    "        })\n",
    "    \n",
    "    # Summary table\n",
    "    print(f\"\\n📊 Progressive Loading Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Resolution':<10} {'LAZ Points':<12} {'LAZ Time':<10} {'COPC Points':<12} {'COPC Time':<10} {'Speedup':<8} {'Reduction':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"{result['resolution']:<10} {result['laz_points']:<12,} {result['laz_time']:<10.2f} \"\n",
    "              f\"{result['copc_points']:<12,} {result['copc_time']:<10.2f} {result['speedup']:<8.1f} \"\n",
    "              f\"{result['data_reduction']:<10.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def benchmark_detail_levels_comparison(laz_file: str, copc_file: str):\n",
    "    \"\"\"Compare different detail levels side by side\"\"\"\n",
    "    \n",
    "    print(f\"\\n🎯 Detail Level Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test with a larger region to see more dramatic differences\n",
    "    region_size = 500.0\n",
    "    \n",
    "    # Get bounds\n",
    "    copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "        \"pipeline\": [\n",
    "            {\"type\": \"readers.copc\", \"filename\": copc_file}\n",
    "        ]\n",
    "    }))\n",
    "    copc_pipeline.execute()\n",
    "    points = copc_pipeline.arrays[0]\n",
    "    \n",
    "    x_min, x_max = float(points['X'].min()), float(points['X'].max())\n",
    "    y_min, y_max = float(points['Y'].min()), float(points['Y'].max())\n",
    "    \n",
    "    region_x_min, region_x_max = x_min, x_min + region_size\n",
    "    region_y_min, region_y_max = y_min, y_min + region_size\n",
    "    \n",
    "    # Test three detail levels: high, medium, low\n",
    "    detail_levels = [\n",
    "        (\"High Detail\", 0.5),\n",
    "        (\"Medium Detail\", 2.0),\n",
    "        (\"Low Detail\", 10.0)\n",
    "    ]\n",
    "    \n",
    "    print(f\"📍 Region: {region_size}m x {region_size}m\")\n",
    "    print(f\"🎯 Testing three detail levels...\")\n",
    "    \n",
    "    for detail_name, resolution in detail_levels:\n",
    "        print(f\"\\n📏 {detail_name} ({resolution}m resolution):\")\n",
    "        \n",
    "        # LAZ (always full detail)\n",
    "        start_time = time.time()\n",
    "        laz_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\"type\": \"readers.las\", \"filename\": laz_file},\n",
    "                {\"type\": \"filters.crop\", \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\"}\n",
    "            ]\n",
    "        }))\n",
    "        laz_pipeline.execute()\n",
    "        laz_time = time.time() - start_time\n",
    "        laz_points = len(laz_pipeline.arrays[0])\n",
    "        \n",
    "        # COPC (progressive detail)\n",
    "        start_time = time.time()\n",
    "        copc_pipeline = pdal.Pipeline(json.dumps({\n",
    "            \"pipeline\": [\n",
    "                {\n",
    "                    \"type\": \"readers.copc\",\n",
    "                    \"filename\": copc_file,\n",
    "                    \"bounds\": f\"([{region_x_min},{region_x_max}],[{region_y_min},{region_y_max}])\",\n",
    "                    \"resolution\": resolution\n",
    "                }\n",
    "            ]\n",
    "        }))\n",
    "        copc_pipeline.execute()\n",
    "        copc_time = time.time() - start_time\n",
    "        copc_points = len(copc_pipeline.arrays[0])\n",
    "        \n",
    "        speedup = laz_time / copc_time if copc_time > 0 else float('inf')\n",
    "        data_reduction = (1 - copc_points/laz_points) * 100 if laz_points > 0 else 0\n",
    "        \n",
    "        print(f\"  LAZ:  {laz_points:,} points in {laz_time:.2f}s\")\n",
    "        print(f\"  COPC: {copc_points:,} points in {copc_time:.2f}s\")\n",
    "        print(f\"  🏆 Speedup: {speedup:.1f}x\")\n",
    "        print(f\"  📉 Data reduction: {data_reduction:.1f}%\")\n",
    "        print(f\"  ⚡ Time saved: {laz_time - copc_time:.2f}s\")\n",
    "\n",
    "def show_progressive_benefits():\n",
    "    \"\"\"Show the benefits of progressive loading\"\"\"\n",
    "    print(f\"\\n🎯 Progressive Loading Benefits:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  ✅ LAZ: Always loads full detail (no choice)\")\n",
    "    print(f\"  ✅ COPC: Can load different detail levels\")\n",
    "    print(f\"  🚀 Faster loading at lower detail levels\")\n",
    "    print(f\"  �� Perfect for mobile/web applications\")\n",
    "    print(f\"  🎮 Great for real-time visualization\")\n",
    "    print(f\"  💾 Bandwidth savings for web services\")\n",
    "    print(f\"  📊 Can start with overview, then add detail\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    laz_file = \"test.laz\"\n",
    "    copc_file = \"copc_output.copc\"\n",
    "    \n",
    "    # Progressive loading benchmark\n",
    "    progressive_results = benchmark_progressive_loading(laz_file, copc_file)\n",
    "    \n",
    "    # Detail level comparison\n",
    "    benchmark_detail_levels_comparison(laz_file, copc_file)\n",
    "    \n",
    "    # Show benefits\n",
    "    show_progressive_benefits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
